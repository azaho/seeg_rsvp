{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zaho/orcd/pool/seeg_rsvp/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from abc import ABC, abstractmethod\n",
    "from transformers import AutoProcessor, AutoModel, CLIPModel\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import screen_setup\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ============================================================\n",
    "#  Base Abstract Class\n",
    "# ============================================================\n",
    "class FeatureExtractor(ABC):\n",
    "    def __init__(self, model_name: str, size: int = 224):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        self.size = size\n",
    "        self.load_model()\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def extract_features(self, img_np):\n",
    "        pass\n",
    "\n",
    "    def process_partial(self, framedata, cross=True, batch_size=64, num_workers=8):\n",
    "        \"\"\"Extract features for frames in batches for efficiency.\"\"\"\n",
    "        if self.model is not None:\n",
    "            self.model.eval()\n",
    "        \n",
    "        # Pre-allocate output array to avoid list growth overhead\n",
    "        features = None\n",
    "        size = self.size\n",
    "        \n",
    "        def load_image(frame):\n",
    "            \"\"\"Load and preprocess a single image (runs in thread).\"\"\"\n",
    "            img_np = screen_setup.preprocess_image(frame, cross=cross, force_size=size)\n",
    "            img_np = cv2.cvtColor(img_np, cv2.COLOR_BGR2RGB)\n",
    "            return img_np\n",
    "\n",
    "        with torch.no_grad():\n",
    "            batch_indices = list(range(0, len(framedata), batch_size))\n",
    "            \n",
    "            # Use thread pool for parallel image loading (I/O bound)\n",
    "            with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "                for i, batch_start in enumerate(tqdm(batch_indices, \n",
    "                                         desc=f\"Extracting {self.model_name} features (batches of {batch_size})\", \n",
    "                                         leave=False)):\n",
    "                    batch_frames = framedata[batch_start:batch_start + batch_size]\n",
    "                    \n",
    "                    # Load images in parallel using threads\n",
    "                    batch_imgs = list(executor.map(load_image, batch_frames))\n",
    "                    \n",
    "                    # Extract features for the batch (GPU)\n",
    "                    batch_feats = self.extract_features_batch(batch_imgs)\n",
    "                    \n",
    "                    # Initialize output array on first batch\n",
    "                    if features is None:\n",
    "                        feat_dim = batch_feats[0].shape[0] if batch_feats[0].ndim == 1 else batch_feats[0].shape\n",
    "                        features = np.zeros((len(framedata), *([feat_dim] if isinstance(feat_dim, int) else feat_dim)), dtype=np.float32)\n",
    "                    \n",
    "                    # Copy directly into pre-allocated array\n",
    "                    for j, feat in enumerate(batch_feats):\n",
    "                        features[batch_start + j] = feat\n",
    "                    \n",
    "                    del batch_imgs, batch_feats\n",
    "\n",
    "        return features\n",
    "    \n",
    "    def extract_features_batch(self, img_list):\n",
    "        \"\"\"Default: process one at a time. Subclasses can override for true batching.\"\"\"\n",
    "        return [self.extract_features(img) for img in img_list]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#  Generic Hugging Face Vision Model Extractor\n",
    "# ============================================================\n",
    "class HFVisionFeatureExtractor(FeatureExtractor):\n",
    "    def load_model(self):\n",
    "        print(f\"Loading model: {self.model_name}\")\n",
    "        self.processor = AutoProcessor.from_pretrained(self.model_name)\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(self.model_name).to(device)\n",
    "\n",
    "    def extract_features(self, img_np):\n",
    "        # Single image version (for compatibility)\n",
    "        return self.extract_features_batch([img_np])[0]\n",
    "    \n",
    "    def extract_features_batch(self, img_list):\n",
    "        \"\"\"Extract features for a batch of images at once.\"\"\"\n",
    "        # Convert numpy arrays to PIL images\n",
    "        pil_images = [Image.fromarray(np.uint8(img)) for img in img_list]\n",
    "        \n",
    "        # Process all images in one call\n",
    "        inputs = self.processor(images=pil_images, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        if \"clip\" in self.model_name.lower():\n",
    "            feats = self.model.get_image_features(pixel_values=inputs[\"pixel_values\"])\n",
    "        elif \"resnet\" in self.model_name.lower():\n",
    "            outputs = self.model(**inputs, output_hidden_states=True)\n",
    "            features = outputs.hidden_states[-1]\n",
    "            features = torch.nn.functional.adaptive_avg_pool2d(features, (1, 1))\n",
    "            feats = features.view(features.size(0), -1)\n",
    "            del outputs, features\n",
    "        else:\n",
    "            outputs = self.model(**inputs)\n",
    "            if hasattr(outputs, \"image_embeds\"):\n",
    "                feats = outputs.image_embeds\n",
    "            # Handle pure vision encoders (ViT, DINO, ConvNeXt)\n",
    "            elif hasattr(outputs, \"pooler_output\"):\n",
    "                feats = outputs.pooler_output\n",
    "            elif hasattr(outputs, \"last_hidden_state\"):\n",
    "                feats = outputs.last_hidden_state.mean(dim=1)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported output type for model: {self.model_name}\")\n",
    "\n",
    "            feats = feats / feats.norm(p=2, dim=-1, keepdim=True)\n",
    "            del outputs\n",
    "        \n",
    "        # Convert to numpy and free GPU tensors\n",
    "        result = [f.cpu().numpy() for f in feats]\n",
    "        del inputs, feats, pil_images\n",
    "        \n",
    "        return result\n",
    "\n",
    "class FlatImageFeatureExtractor(FeatureExtractor):\n",
    "    \"\"\"Feature extractor that resizes the image to (size, size), flattens. Handles color or grayscale.\"\"\"\n",
    "    def __init__(self, model_name=\"raw_image\", size=224):\n",
    "        super().__init__(model_name)\n",
    "        self.size = size\n",
    "        self.model = None\n",
    "\n",
    "    def load_model(self):\n",
    "        # No model to load\n",
    "        pass\n",
    "\n",
    "    def extract_features(self, img_np):\n",
    "        # img_np: HxWxC numpy array (expects HWC, uint8 or float)\n",
    "        # Resize to (size, size)\n",
    "        img_resized = cv2.resize(img_np, (self.size, self.size), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        if img_resized.ndim == 2:\n",
    "            # Grayscale image, shape (size, size)\n",
    "            flat = img_resized.flatten().astype(np.float32)\n",
    "        elif img_resized.ndim == 3:\n",
    "            # Color image, shape (size, size, C)\n",
    "            # Flatten all channels (row-major, then channel per pixel)\n",
    "            flat = img_resized.flatten().astype(np.float32)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported image shape after resize: {img_resized.shape}\")\n",
    "\n",
    "        return flat\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#  Adversarial Training Resnet50\n",
    "# ============================================================\n",
    "\n",
    "import at_resnet50\n",
    "import dill\n",
    "import torch\n",
    "\n",
    "ch = torch\n",
    "class InputNormalize(ch.nn.Module):\n",
    "    '''\n",
    "    A module (custom layer) for normalizing the input to have a fixed \n",
    "    mean and standard deviation (user-specified).\n",
    "    '''\n",
    "    def __init__(self, new_mean, new_std):\n",
    "        super(InputNormalize, self).__init__()\n",
    "        new_std = new_std[..., None, None]\n",
    "        new_mean = new_mean[..., None, None]\n",
    "\n",
    "        self.register_buffer(\"new_mean\", new_mean)\n",
    "        self.register_buffer(\"new_std\", new_std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = ch.clamp(x, 0, 1)\n",
    "        x_normalized = (x - self.new_mean)/self.new_std\n",
    "        return x_normalized\n",
    "class ATResnet50(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = at_resnet50.resnet50(pretrained=False)\n",
    "        self.normalizer = InputNormalize(new_mean=torch.tensor([0.485, 0.456, 0.406]), new_std=torch.tensor([0.229, 0.224, 0.225]))\n",
    "\n",
    "    def forward(self, x, with_latent=False):\n",
    "        x = self.normalizer(x)\n",
    "        x, latent = self.model(x, with_latent=with_latent)\n",
    "        return x, latent\n",
    "\n",
    "\n",
    "at_resnet_50 = ATResnet50()\n",
    "checkpoint = torch.load(\"_dataset_zip/imagenet_l2_3_0.pt\", pickle_module=dill, map_location=torch.device('cpu'))\n",
    "# Makes us able to load models saved with legacy versions\n",
    "state_dict_path = 'model'\n",
    "if not ('model' in checkpoint):\n",
    "    state_dict_path = 'state_dict'\n",
    "sd = checkpoint[state_dict_path]\n",
    "sd = {k[len('module.'):]:v for k,v in sd.items() if 'attacker' not in k}\n",
    "at_resnet_50.load_state_dict(sd)\n",
    "at_resnet_50 = at_resnet_50.to(device)\n",
    "\n",
    "class ATResnet50FeatureExtractor(FeatureExtractor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if \"model_name\" not in kwargs:\n",
    "            kwargs[\"model_name\"] = \"at_resnet_50\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # at_resnet_50 is globally available from the top initialization; else, could instantiate here\n",
    "        self.model = at_resnet_50\n",
    "        self.model.eval()\n",
    "\n",
    "    def load_model(self):\n",
    "        # Model is already loaded in __init__\n",
    "        pass\n",
    "\n",
    "    def extract_features(self, img_np):\n",
    "        # Single image version (for compatibility)\n",
    "        return self.extract_features_batch([img_np])[0]\n",
    "    \n",
    "    def extract_features_batch(self, img_list):\n",
    "        \"\"\"Extract features for a batch of images at once.\"\"\"\n",
    "        # Convert all images to tensors and stack\n",
    "        batch_tensors = []\n",
    "        for img_np in img_list:\n",
    "            # img_np is in RGB, 0-255\n",
    "            img_t = torch.from_numpy(img_np).float() / 255.0\n",
    "            img_t = img_t.permute(2, 0, 1)  # (3, H, W)\n",
    "            batch_tensors.append(img_t)\n",
    "        \n",
    "        # Stack into batch: (N, 3, H, W)\n",
    "        batch = torch.stack(batch_tensors).to(device)\n",
    "        \n",
    "        # No gradients needed\n",
    "        with torch.no_grad():\n",
    "            output, latent = self.model(batch, with_latent=True)\n",
    "        \n",
    "        # Convert to numpy and free GPU tensors\n",
    "        result = [f.cpu().numpy() for f in latent]\n",
    "        del batch, output, latent, batch_tensors\n",
    "        \n",
    "        return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: openai/clip-vit-base-patch32\n",
      "Loading model: facebook/dino-vits16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/dino-vits16 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: google/vit-base-patch16-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: microsoft/resnet-50\n",
      "\n",
      "üìÅ Processing: OASIS\n",
      "‚Üí Extracting openai/clip-vit-base-patch32 features in chunks of 5000 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved chunk 1 [0:807] to imagebank/OASIS/features_openai_clip-vit-base-patch32_part1.npy\n",
      "‚Üí Combining 1 parts into one final npy...\n",
      "‚úÖ Saved combined: imagebank/OASIS/features_openai_clip-vit-base-patch32.npy\n",
      "‚Üí Skipping facebook/dino-vits16 features for OASIS because they already exist\n",
      "‚Üí Skipping google/vit-base-patch16-224 features for OASIS because they already exist\n",
      "‚Üí Skipping microsoft/resnet-50 features for OASIS because they already exist\n",
      "‚Üí Extracting at_resnet_50 features in chunks of 5000 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved chunk 1 [0:807] to imagebank/OASIS/features_at_resnet_50_part1.npy\n",
      "‚Üí Combining 1 parts into one final npy...\n",
      "‚úÖ Saved combined: imagebank/OASIS/features_at_resnet_50.npy\n",
      "\n",
      "üìÅ Processing: MIT003_single_2_drivesuppress_s1_n480_x4_on100-100_off125-175\n",
      "‚Üí Skipping openai/clip-vit-base-patch32 features for MIT003_single_2_drivesuppress_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping facebook/dino-vits16 features for MIT003_single_2_drivesuppress_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping google/vit-base-patch16-224 features for MIT003_single_2_drivesuppress_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping microsoft/resnet-50 features for MIT003_single_2_drivesuppress_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping at_resnet_50 features for MIT003_single_2_drivesuppress_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "\n",
      "üìÅ Processing: MIT003_single_1_drivesuppress_s1_n480_x4_on100-100_off125-175\n",
      "‚Üí Skipping openai/clip-vit-base-patch32 features for MIT003_single_1_drivesuppress_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping facebook/dino-vits16 features for MIT003_single_1_drivesuppress_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping google/vit-base-patch16-224 features for MIT003_single_1_drivesuppress_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping microsoft/resnet-50 features for MIT003_single_1_drivesuppress_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping at_resnet_50 features for MIT003_single_1_drivesuppress_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "\n",
      "üìÅ Processing: drive_suppress_MIT002_1_s1_n480_x4_on100-100_off125-175\n",
      "‚Üí Skipping openai/clip-vit-base-patch32 features for drive_suppress_MIT002_1_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping facebook/dino-vits16 features for drive_suppress_MIT002_1_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping google/vit-base-patch16-224 features for drive_suppress_MIT002_1_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping microsoft/resnet-50 features for drive_suppress_MIT002_1_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping at_resnet_50 features for drive_suppress_MIT002_1_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "\n",
      "üìÅ Processing: trial_s1_n480_x4_on100-100_off125-175\n",
      "‚Üí Skipping openai/clip-vit-base-patch32 features for trial_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping facebook/dino-vits16 features for trial_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping google/vit-base-patch16-224 features for trial_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping microsoft/resnet-50 features for trial_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping at_resnet_50 features for trial_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "\n",
      "üìÅ Processing: drive_suppress_MIT002_2_s1_n480_x4_on100-100_off125-175\n",
      "‚Üí Skipping openai/clip-vit-base-patch32 features for drive_suppress_MIT002_2_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping facebook/dino-vits16 features for drive_suppress_MIT002_2_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping google/vit-base-patch16-224 features for drive_suppress_MIT002_2_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping microsoft/resnet-50 features for drive_suppress_MIT002_2_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping at_resnet_50 features for drive_suppress_MIT002_2_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "\n",
      "üìÅ Processing: trial_s3_n960_x2_on100-100_off125-175\n",
      "‚Üí Skipping openai/clip-vit-base-patch32 features for trial_s3_n960_x2_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping facebook/dino-vits16 features for trial_s3_n960_x2_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping google/vit-base-patch16-224 features for trial_s3_n960_x2_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping microsoft/resnet-50 features for trial_s3_n960_x2_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping at_resnet_50 features for trial_s3_n960_x2_on100-100_off125-175 because they already exist\n",
      "\n",
      "üìÅ Processing: ILSVRC2012_img_val\n",
      "‚Üí Skipping openai/clip-vit-base-patch32 features for ILSVRC2012_img_val because they already exist\n",
      "‚Üí Skipping facebook/dino-vits16 features for ILSVRC2012_img_val because they already exist\n",
      "‚Üí Skipping google/vit-base-patch16-224 features for ILSVRC2012_img_val because they already exist\n",
      "‚Üí Skipping microsoft/resnet-50 features for ILSVRC2012_img_val because they already exist\n",
      "‚Üí Skipping at_resnet_50 features for ILSVRC2012_img_val because they already exist\n",
      "\n",
      "üìÅ Processing: TrailerFacesHQ\n",
      "‚Üí Skipping openai/clip-vit-base-patch32 features for TrailerFacesHQ because they already exist\n",
      "‚Üí Skipping facebook/dino-vits16 features for TrailerFacesHQ because they already exist\n",
      "‚Üí Skipping google/vit-base-patch16-224 features for TrailerFacesHQ because they already exist\n",
      "‚Üí Skipping microsoft/resnet-50 features for TrailerFacesHQ because they already exist\n",
      "‚Üí Skipping at_resnet_50 features for TrailerFacesHQ because they already exist\n",
      "\n",
      "üìÅ Processing: trial_s2_n960_x2_on100-100_off125-175\n",
      "‚Üí Skipping openai/clip-vit-base-patch32 features for trial_s2_n960_x2_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping facebook/dino-vits16 features for trial_s2_n960_x2_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping google/vit-base-patch16-224 features for trial_s2_n960_x2_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping microsoft/resnet-50 features for trial_s2_n960_x2_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping at_resnet_50 features for trial_s2_n960_x2_on100-100_off125-175 because they already exist\n",
      "\n",
      "üìÅ Processing: tutorial_sT_n160_x2_on100-100_off375-425\n",
      "‚Üí Skipping openai/clip-vit-base-patch32 features for tutorial_sT_n160_x2_on100-100_off375-425 because they already exist\n",
      "‚Üí Skipping facebook/dino-vits16 features for tutorial_sT_n160_x2_on100-100_off375-425 because they already exist\n",
      "‚Üí Skipping google/vit-base-patch16-224 features for tutorial_sT_n160_x2_on100-100_off375-425 because they already exist\n",
      "‚Üí Skipping microsoft/resnet-50 features for tutorial_sT_n160_x2_on100-100_off375-425 because they already exist\n",
      "‚Üí Skipping at_resnet_50 features for tutorial_sT_n160_x2_on100-100_off375-425 because they already exist\n",
      "\n",
      "üìÅ Processing: trial_s1_n960_x2_on100-100_off125-175\n",
      "‚Üí Skipping openai/clip-vit-base-patch32 features for trial_s1_n960_x2_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping facebook/dino-vits16 features for trial_s1_n960_x2_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping google/vit-base-patch16-224 features for trial_s1_n960_x2_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping microsoft/resnet-50 features for trial_s1_n960_x2_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping at_resnet_50 features for trial_s1_n960_x2_on100-100_off125-175 because they already exist\n",
      "\n",
      "üìÅ Processing: MIT003_single_1_drivesuppress_reshuffle_s1_n480_x4_on100-100_off125-175\n",
      "‚Üí Skipping openai/clip-vit-base-patch32 features for MIT003_single_1_drivesuppress_reshuffle_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping facebook/dino-vits16 features for MIT003_single_1_drivesuppress_reshuffle_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping google/vit-base-patch16-224 features for MIT003_single_1_drivesuppress_reshuffle_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping microsoft/resnet-50 features for MIT003_single_1_drivesuppress_reshuffle_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping at_resnet_50 features for MIT003_single_1_drivesuppress_reshuffle_s1_n480_x4_on100-100_off125-175 because they already exist\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "#  Main Pipeline (chunked feature extraction)\n",
    "# ============================================================\n",
    "def process_imagebank(extractorss, imagebank_dirs, imagebank_root=\"imagebank/\", chunk_size=5000):\n",
    "\n",
    "    for imagebank_dir in imagebank_dirs:\n",
    "        print(f\"\\nüìÅ Processing: {imagebank_dir}\")\n",
    "        # Try framedata_features.json, then framedata.json, then template.json\n",
    "        for fname in [\"framedata_features.json\", \"framedata.json\", \"template.json\"]:\n",
    "            path_json = os.path.join(imagebank_root, imagebank_dir, fname)\n",
    "            if os.path.exists(path_json):\n",
    "                break\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No framedata JSON found in {os.path.join(imagebank_root, imagebank_dir)}\")\n",
    "        with open(path_json, \"r\") as f:\n",
    "            framedata = json.load(f)['framedata']\n",
    "\n",
    "        # Remove all repeats\n",
    "        unique_paths = set()\n",
    "        unique_framedata = []\n",
    "        for frame in framedata:\n",
    "            if frame['image_path'] not in unique_paths:\n",
    "                unique_paths.add(frame['image_path'])\n",
    "                unique_framedata.append(frame)\n",
    "        framedata = unique_framedata\n",
    "        # Save the current framedata as framedata_features.json in the appropriate imagebank_dir\n",
    "        framwdata_features_path = os.path.join(imagebank_root, imagebank_dir, \"framedata_features.json\")\n",
    "        with open(framwdata_features_path, \"w\") as f_out:\n",
    "            json.dump({\"framedata\": framedata}, f_out, indent=2)\n",
    "\n",
    "        for extractor in extractors:\n",
    "            # sanitize name for filesystem\n",
    "            safe_name = extractor.model_name.replace(\"/\", \"_\")\n",
    "            save_path = os.path.join(imagebank_root, imagebank_dir, f\"features_{safe_name}.npy\")\n",
    "            if os.path.exists(save_path):\n",
    "                print(f\"‚Üí Skipping {extractor.model_name} features for {imagebank_dir} because they already exist\")\n",
    "                continue\n",
    "\n",
    "            # Chunked saving\n",
    "            print(f\"‚Üí Extracting {extractor.model_name} features in chunks of {chunk_size} ...\")\n",
    "            n_images = len(framedata)\n",
    "            part_paths = []\n",
    "            for part_idx, start_idx in enumerate(range(0, n_images, chunk_size)):\n",
    "                partpath = os.path.join(imagebank_root, imagebank_dir, f\"features_{safe_name}_part{part_idx+1}.npy\")\n",
    "                if os.path.exists(partpath):\n",
    "                    print(f\"‚Üí Skipping {extractor.model_name} features for {imagebank_dir} because they already exist\")\n",
    "                    continue\n",
    "                end_idx = min(start_idx + chunk_size, n_images)\n",
    "                feats = extractor.process_partial(framedata[start_idx:end_idx])\n",
    "                np.save(partpath, feats)\n",
    "                part_paths.append(partpath)\n",
    "                print(f\"    Saved chunk {part_idx+1} [{start_idx}:{end_idx}] to {partpath}\")\n",
    "                \n",
    "            # Combine\n",
    "            print(f\"‚Üí Combining {len(part_paths)} parts into one final npy...\")\n",
    "            all_parts = [np.load(pp) for pp in part_paths]\n",
    "            full_feats = np.concatenate(all_parts)\n",
    "            np.save(save_path, full_feats)\n",
    "            print(f\"‚úÖ Saved combined: {save_path}\")\n",
    "\n",
    "            # Clean up part files\n",
    "            for pp in part_paths:\n",
    "                os.remove(pp)\n",
    "\n",
    "imagebank_root=\"imagebank/\"\n",
    "imagebank_dirs = [\n",
    "    d for d in os.listdir(imagebank_root)\n",
    "    if os.path.isdir(os.path.join(imagebank_root, d))\n",
    "]\n",
    "# imagebank_dirs = [\"trial_s1_n480_x4_on100-100_off125-175\"]\n",
    "\n",
    "\n",
    "model_list = [\n",
    "    \"openai/clip-vit-base-patch32\",\n",
    "    \"facebook/dino-vits16\",\n",
    "    \"google/vit-base-patch16-224\",\n",
    "    \"microsoft/resnet-50\",\n",
    "]\n",
    "extractors = [HFVisionFeatureExtractor(name) for name in model_list]\n",
    "# extractors.append(FlatImageFeatureExtractor())\n",
    "extractors += [ATResnet50FeatureExtractor()]\n",
    "\n",
    "process_imagebank(extractors, imagebank_dirs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
