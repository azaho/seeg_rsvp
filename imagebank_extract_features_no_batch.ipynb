{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zaho/orcd/pool/seeg_rsvp/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from abc import ABC, abstractmethod\n",
    "from transformers import AutoProcessor, AutoModel, CLIPModel\n",
    "\n",
    "import screen_setup\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# ============================================================\n",
    "#  Base Abstract Class\n",
    "# ============================================================\n",
    "class FeatureExtractor(ABC):\n",
    "    def __init__(self, model_name: str, size: int = 224):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        self.size = size\n",
    "        self.load_model()\n",
    "\n",
    "    @abstractmethod\n",
    "    def load_model(self):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def extract_features(self, img_np):\n",
    "        pass\n",
    "\n",
    "    def process_partial(self, framedata, cross=True):\n",
    "        \"\"\"Extract features for a batch of frames (no stacking).\"\"\"\n",
    "        if self.model is not None:\n",
    "            self.model.eval()\n",
    "        features = []\n",
    "        # print(framedata)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for frame in tqdm(framedata, desc=f\"Extracting {self.model_name} features (partial)\", leave=False):\n",
    "                img_np = screen_setup.preprocess_image(frame, cross=cross, force_size=self.size)\n",
    "                # convert from BGR to RGB\n",
    "                img_np = cv2.cvtColor(img_np, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                feat = self.extract_features(img_np)\n",
    "                features.append(feat)\n",
    "\n",
    "        return np.stack(features)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#  Generic Hugging Face Vision Model Extractor\n",
    "# ============================================================\n",
    "class HFVisionFeatureExtractor(FeatureExtractor):\n",
    "    def load_model(self):\n",
    "        print(f\"Loading model: {self.model_name}\")\n",
    "        self.processor = AutoProcessor.from_pretrained(self.model_name)\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(self.model_name).to(device)\n",
    "\n",
    "    def extract_features(self, img_np):\n",
    "        img_pil = Image.fromarray(np.uint8(img_np))\n",
    "        inputs = self.processor(images=img_pil, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        if \"clip\" in self.model_name.lower():\n",
    "            feats = self.model.get_image_features(pixel_values=inputs[\"pixel_values\"])\n",
    "        elif \"resnet\" in self.model_name.lower():\n",
    "            outputs = self.model(**inputs, output_hidden_states=True)\n",
    "            features = outputs.hidden_states[-1]\n",
    "            features = torch.nn.functional.adaptive_avg_pool2d(features, (1, 1))\n",
    "            feats = features.view(features.size(0), -1)\n",
    "        else:\n",
    "            outputs = self.model(**inputs)\n",
    "            if hasattr(outputs, \"image_embeds\"):\n",
    "                feats = outputs.image_embeds\n",
    "            # Handle pure vision encoders (ViT, DINO, ConvNeXt)\n",
    "            elif hasattr(outputs, \"pooler_output\"):\n",
    "                feats = outputs.pooler_output\n",
    "            elif hasattr(outputs, \"last_hidden_state\"):\n",
    "                feats = outputs.last_hidden_state.mean(dim=1)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported output type for model: {self.model_name}\")\n",
    "\n",
    "            feats = feats / feats.norm(p=2, dim=-1, keepdim=True)\n",
    "        return feats.cpu().numpy().squeeze()\n",
    "\n",
    "class FlatImageFeatureExtractor(FeatureExtractor):\n",
    "    \"\"\"Feature extractor that resizes the image to (size, size), flattens. Handles color or grayscale.\"\"\"\n",
    "    def __init__(self, model_name=\"raw_image\", size=224):\n",
    "        super().__init__(model_name)\n",
    "        self.size = size\n",
    "        self.model = None\n",
    "\n",
    "    def load_model(self):\n",
    "        # No model to load\n",
    "        pass\n",
    "\n",
    "    def extract_features(self, img_np):\n",
    "        # img_np: HxWxC numpy array (expects HWC, uint8 or float)\n",
    "        # Resize to (size, size)\n",
    "        img_resized = cv2.resize(img_np, (self.size, self.size), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        if img_resized.ndim == 2:\n",
    "            # Grayscale image, shape (size, size)\n",
    "            flat = img_resized.flatten().astype(np.float32)\n",
    "        elif img_resized.ndim == 3:\n",
    "            # Color image, shape (size, size, C)\n",
    "            # Flatten all channels (row-major, then channel per pixel)\n",
    "            flat = img_resized.flatten().astype(np.float32)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported image shape after resize: {img_resized.shape}\")\n",
    "\n",
    "        return flat\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#  Adversarial Training Resnet50\n",
    "# ============================================================\n",
    "\n",
    "import at_resnet50\n",
    "import dill\n",
    "import torch\n",
    "\n",
    "ch = torch\n",
    "class InputNormalize(ch.nn.Module):\n",
    "    '''\n",
    "    A module (custom layer) for normalizing the input to have a fixed \n",
    "    mean and standard deviation (user-specified).\n",
    "    '''\n",
    "    def __init__(self, new_mean, new_std):\n",
    "        super(InputNormalize, self).__init__()\n",
    "        new_std = new_std[..., None, None]\n",
    "        new_mean = new_mean[..., None, None]\n",
    "\n",
    "        self.register_buffer(\"new_mean\", new_mean)\n",
    "        self.register_buffer(\"new_std\", new_std)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = ch.clamp(x, 0, 1)\n",
    "        x_normalized = (x - self.new_mean)/self.new_std\n",
    "        return x_normalized\n",
    "class ATResnet50(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = at_resnet50.resnet50(pretrained=False)\n",
    "        self.normalizer = InputNormalize(new_mean=torch.tensor([0.485, 0.456, 0.406]), new_std=torch.tensor([0.229, 0.224, 0.225]))\n",
    "\n",
    "    def forward(self, x, with_latent=False):\n",
    "        x = self.normalizer(x)\n",
    "        x, latent = self.model(x, with_latent=with_latent)\n",
    "        return x, latent\n",
    "\n",
    "\n",
    "at_resnet_50 = ATResnet50()\n",
    "checkpoint = torch.load(\"_dataset_zip/imagenet_l2_3_0.pt\", pickle_module=dill, map_location=torch.device('cpu'))\n",
    "# Makes us able to load models saved with legacy versions\n",
    "state_dict_path = 'model'\n",
    "if not ('model' in checkpoint):\n",
    "    state_dict_path = 'state_dict'\n",
    "sd = checkpoint[state_dict_path]\n",
    "sd = {k[len('module.'):]:v for k,v in sd.items() if 'attacker' not in k}\n",
    "at_resnet_50.load_state_dict(sd)\n",
    "at_resnet_50 = at_resnet_50.to(device)\n",
    "\n",
    "class ATResnet50FeatureExtractor(FeatureExtractor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if \"model_name\" not in kwargs:\n",
    "            kwargs[\"model_name\"] = \"at_resnet_50\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # at_resnet_50 is globally available from the top initialization; else, could instantiate here\n",
    "        self.model = at_resnet_50\n",
    "        self.model.eval()\n",
    "\n",
    "    def load_model(self):\n",
    "        # Model is already loaded in __init__\n",
    "        pass\n",
    "\n",
    "    def extract_features(self, img_np):\n",
    "        # img_np is in RGB, 0-255\n",
    "        # Convert to torch tensor and normalize to [0, 1]\n",
    "        img_t = torch.from_numpy(img_np).float() / 255.0\n",
    "        img_t = img_t.permute(2, 0, 1).unsqueeze(0).to(device)  # (1, 3, H, W)\n",
    "\n",
    "        # No gradients needed\n",
    "        with torch.no_grad():\n",
    "            output, latent = self.model(img_t, with_latent=True)\n",
    "        # latent is the penultimate features (before fc)\n",
    "        feats = latent\n",
    "        return feats.cpu().numpy().squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: openai/clip-vit-base-patch32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: facebook/dino-vits16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at facebook/dino-vits16 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: google/vit-base-patch16-224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: microsoft/resnet-50\n",
      "\n",
      "üìÅ Processing: OASIS\n",
      "‚Üí Skipping openai/clip-vit-base-patch32 features for OASIS because they already exist\n",
      "‚Üí Skipping facebook/dino-vits16 features for OASIS because they already exist\n",
      "‚Üí Skipping google/vit-base-patch16-224 features for OASIS because they already exist\n",
      "‚Üí Skipping microsoft/resnet-50 features for OASIS because they already exist\n",
      "‚Üí Skipping at_resnet_50 features for OASIS because they already exist\n",
      "\n",
      "üìÅ Processing: MIT003_single_2_drivesuppress_s1_n480_x4_on100-100_off125-175\n",
      "‚Üí Skipping openai/clip-vit-base-patch32 features for MIT003_single_2_drivesuppress_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping facebook/dino-vits16 features for MIT003_single_2_drivesuppress_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping google/vit-base-patch16-224 features for MIT003_single_2_drivesuppress_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping microsoft/resnet-50 features for MIT003_single_2_drivesuppress_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping at_resnet_50 features for MIT003_single_2_drivesuppress_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "\n",
      "üìÅ Processing: MIT003_single_1_drivesuppress_s1_n480_x4_on100-100_off125-175\n",
      "‚Üí Skipping openai/clip-vit-base-patch32 features for MIT003_single_1_drivesuppress_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping facebook/dino-vits16 features for MIT003_single_1_drivesuppress_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping google/vit-base-patch16-224 features for MIT003_single_1_drivesuppress_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping microsoft/resnet-50 features for MIT003_single_1_drivesuppress_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping at_resnet_50 features for MIT003_single_1_drivesuppress_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "\n",
      "üìÅ Processing: drive_suppress_MIT002_1_s1_n480_x4_on100-100_off125-175\n",
      "‚Üí Skipping openai/clip-vit-base-patch32 features for drive_suppress_MIT002_1_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping facebook/dino-vits16 features for drive_suppress_MIT002_1_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping google/vit-base-patch16-224 features for drive_suppress_MIT002_1_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping microsoft/resnet-50 features for drive_suppress_MIT002_1_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping at_resnet_50 features for drive_suppress_MIT002_1_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "\n",
      "üìÅ Processing: trial_s1_n480_x4_on100-100_off125-175\n",
      "‚Üí Skipping openai/clip-vit-base-patch32 features for trial_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping facebook/dino-vits16 features for trial_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping google/vit-base-patch16-224 features for trial_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping microsoft/resnet-50 features for trial_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping at_resnet_50 features for trial_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "\n",
      "üìÅ Processing: drive_suppress_MIT002_2_s1_n480_x4_on100-100_off125-175\n",
      "‚Üí Skipping openai/clip-vit-base-patch32 features for drive_suppress_MIT002_2_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping facebook/dino-vits16 features for drive_suppress_MIT002_2_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping google/vit-base-patch16-224 features for drive_suppress_MIT002_2_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping microsoft/resnet-50 features for drive_suppress_MIT002_2_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "‚Üí Skipping at_resnet_50 features for drive_suppress_MIT002_2_s1_n480_x4_on100-100_off125-175 because they already exist\n",
      "\n",
      "üìÅ Processing: trial_s3_n960_x2_on100-100_off125-175\n",
      "‚Üí Extracting openai/clip-vit-base-patch32 features in chunks of 5000 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved chunk 1 [0:960] to imagebank/trial_s3_n960_x2_on100-100_off125-175/features_openai_clip-vit-base-patch32_part1.npy\n",
      "‚Üí Combining 1 parts into one final npy...\n",
      "‚úÖ Saved combined: imagebank/trial_s3_n960_x2_on100-100_off125-175/features_openai_clip-vit-base-patch32.npy\n",
      "‚Üí Extracting facebook/dino-vits16 features in chunks of 5000 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved chunk 1 [0:960] to imagebank/trial_s3_n960_x2_on100-100_off125-175/features_facebook_dino-vits16_part1.npy\n",
      "‚Üí Combining 1 parts into one final npy...\n",
      "‚úÖ Saved combined: imagebank/trial_s3_n960_x2_on100-100_off125-175/features_facebook_dino-vits16.npy\n",
      "‚Üí Extracting google/vit-base-patch16-224 features in chunks of 5000 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved chunk 1 [0:960] to imagebank/trial_s3_n960_x2_on100-100_off125-175/features_google_vit-base-patch16-224_part1.npy\n",
      "‚Üí Combining 1 parts into one final npy...\n",
      "‚úÖ Saved combined: imagebank/trial_s3_n960_x2_on100-100_off125-175/features_google_vit-base-patch16-224.npy\n",
      "‚Üí Extracting microsoft/resnet-50 features in chunks of 5000 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved chunk 1 [0:960] to imagebank/trial_s3_n960_x2_on100-100_off125-175/features_microsoft_resnet-50_part1.npy\n",
      "‚Üí Combining 1 parts into one final npy...\n",
      "‚úÖ Saved combined: imagebank/trial_s3_n960_x2_on100-100_off125-175/features_microsoft_resnet-50.npy\n",
      "‚Üí Extracting at_resnet_50 features in chunks of 5000 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved chunk 1 [0:960] to imagebank/trial_s3_n960_x2_on100-100_off125-175/features_at_resnet_50_part1.npy\n",
      "‚Üí Combining 1 parts into one final npy...\n",
      "‚úÖ Saved combined: imagebank/trial_s3_n960_x2_on100-100_off125-175/features_at_resnet_50.npy\n",
      "\n",
      "üìÅ Processing: ILSVRC2012_img_val\n",
      "‚Üí Extracting openai/clip-vit-base-patch32 features in chunks of 5000 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Saved chunk 1 [0:5000] to imagebank/ILSVRC2012_img_val/features_openai_clip-vit-base-patch32_part1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting openai/clip-vit-base-patch32 features (partial):   9%|‚ñâ         | 466/5000 [01:00<13:08,  5.75it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "#  Main Pipeline (chunked feature extraction)\n",
    "# ============================================================\n",
    "def process_imagebank(extractorss, imagebank_dirs, imagebank_root=\"imagebank/\", chunk_size=5000):\n",
    "\n",
    "    for imagebank_dir in imagebank_dirs:\n",
    "        print(f\"\\nüìÅ Processing: {imagebank_dir}\")\n",
    "        # Try framedata_features.json, then framedata.json, then template.json\n",
    "        for fname in [\"framedata_features.json\", \"framedata.json\", \"template.json\"]:\n",
    "            path_json = os.path.join(imagebank_root, imagebank_dir, fname)\n",
    "            if os.path.exists(path_json):\n",
    "                break\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"No framedata JSON found in {os.path.join(imagebank_root, imagebank_dir)}\")\n",
    "        with open(path_json, \"r\") as f:\n",
    "            framedata = json.load(f)['framedata']\n",
    "\n",
    "        # Remove all repeats\n",
    "        unique_paths = set()\n",
    "        unique_framedata = []\n",
    "        for frame in framedata:\n",
    "            if frame['image_path'] not in unique_paths:\n",
    "                unique_paths.add(frame['image_path'])\n",
    "                unique_framedata.append(frame)\n",
    "        framedata = unique_framedata\n",
    "        # Save the current framedata as framedata_features.json in the appropriate imagebank_dir\n",
    "        framwdata_features_path = os.path.join(imagebank_root, imagebank_dir, \"framedata_features.json\")\n",
    "        with open(framwdata_features_path, \"w\") as f_out:\n",
    "            json.dump({\"framedata\": framedata}, f_out, indent=2)\n",
    "\n",
    "        for extractor in extractors:\n",
    "            # sanitize name for filesystem\n",
    "            safe_name = extractor.model_name.replace(\"/\", \"_\")\n",
    "            save_path = os.path.join(imagebank_root, imagebank_dir, f\"features_{safe_name}.npy\")\n",
    "            if os.path.exists(save_path):\n",
    "                print(f\"‚Üí Skipping {extractor.model_name} features for {imagebank_dir} because they already exist\")\n",
    "                continue\n",
    "\n",
    "            # Chunked saving\n",
    "            print(f\"‚Üí Extracting {extractor.model_name} features in chunks of {chunk_size} ...\")\n",
    "            n_images = len(framedata)\n",
    "            part_paths = []\n",
    "            for part_idx, start_idx in enumerate(range(0, n_images, chunk_size)):\n",
    "                partpath = os.path.join(imagebank_root, imagebank_dir, f\"features_{safe_name}_part{part_idx+1}.npy\")\n",
    "                if os.path.exists(partpath):\n",
    "                    print(f\"‚Üí Skipping {extractor.model_name} features for {imagebank_dir} because they already exist\")\n",
    "                    continue\n",
    "                end_idx = min(start_idx + chunk_size, n_images)\n",
    "                feats = extractor.process_partial(framedata[start_idx:end_idx])\n",
    "                np.save(partpath, feats)\n",
    "                part_paths.append(partpath)\n",
    "                print(f\"    Saved chunk {part_idx+1} [{start_idx}:{end_idx}] to {partpath}\")\n",
    "                \n",
    "            # Combine\n",
    "            print(f\"‚Üí Combining {len(part_paths)} parts into one final npy...\")\n",
    "            all_parts = [np.load(pp) for pp in part_paths]\n",
    "            full_feats = np.concatenate(all_parts)\n",
    "            np.save(save_path, full_feats)\n",
    "            print(f\"‚úÖ Saved combined: {save_path}\")\n",
    "\n",
    "            # Clean up part files\n",
    "            for pp in part_paths:\n",
    "                os.remove(pp)\n",
    "\n",
    "imagebank_root=\"imagebank/\"\n",
    "imagebank_dirs = [\n",
    "    d for d in os.listdir(imagebank_root)\n",
    "    if os.path.isdir(os.path.join(imagebank_root, d))\n",
    "]\n",
    "# imagebank_dirs = [\"trial_s1_n480_x4_on100-100_off125-175\"]\n",
    "\n",
    "\n",
    "model_list = [\n",
    "    \"openai/clip-vit-base-patch32\",\n",
    "    \"facebook/dino-vits16\",\n",
    "    \"google/vit-base-patch16-224\",\n",
    "    \"microsoft/resnet-50\",\n",
    "]\n",
    "extractors = [HFVisionFeatureExtractor(name) for name in model_list]\n",
    "# extractors.append(FlatImageFeatureExtractor())\n",
    "extractors += [ATResnet50FeatureExtractor()]\n",
    "\n",
    "process_imagebank(extractors, imagebank_dirs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
